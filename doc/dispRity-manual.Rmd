---
title: "dispRity manual"
author: "Thomas Guillerme"
date: "`r Sys.Date()`"
output:
    rmarkdown::html_vignette:
        toc: true
        toc_depth: 2
bibliography: References.bib
bst: sysbio.bst
vignette: >
  %\VignetteIndexEntry{dispRity manual}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

# `dispRity`

This is a package for measuring disparity in `R`.
It allows users to summarise ordinated matrices (e.g. MDS, PCA, PCO, PCoA) into single values so they can easily be compared.
This manual is based on the version `1.0`.

# Introduction

## What is `dispRity`?
This is a package for measuring disparity in `R`.
It allows users to summarise ordinated matrices (e.g. MDS, PCA, PCO, PCoA) to perform some multidimensional analysis.
Typically, these analysis are used in palaeobiology and evolutionary biology to study the changes in morphology through time.
However, there are many more applications in ecology, evolution and beyond.


## Installing and running the package

You can install this package easily if you use the latest version of `R` (> 3.3.0) and `devtools`.

```{r, eval=FALSE}
## Checking if devtools is already installed
if(!require(devtools)) install.packages("devtools")

## Installing the latest released version directly from GitHub
install_github("TGuillerme/dispRity", ref = "release")

## Loading the package
library(dispRity)
```

```{r, eval = TRUE, echo = FALSE, message = FALSE}
## Checking if Claddis is installed BUILD ONLY
# NC: this isn't checking Claddis...
library(dispRity)
## Setting the random seed for repeatability
set.seed(123)
```

Note this uses the `release` branch (version`0.4`). <!-- @@@ -->
For the piping-hot (but potentially unstable) version, you can change the argument `ref = release` to `ref = master`.
`dispRity` depends mainly on the `ape` package and uses functions from several other packages (`ade4`, `geometry`, `grDevices`, `hypervolume`, `paleotree`, `snow`, `Claddis`, `geomorph` and `RCurl`).

## Why not CRAN?

This package is not available on CRAN.
This is mainly because some parts are still in development and that the reactivity of GitHub is better for implementing new suggestions from users.
However, the package follows the strict (and useful!) CRAN standards via Travis.

[![Build Status](https://travis-ci.org/TGuillerme/dispRity.svg?branch=release)](https://travis-ci.org/TGuillerme/dispRity)
[![codecov](https://codecov.io/gh/TGuillerme/dispRity/branch/release/graph/badge.svg)](https://codecov.io/gh/TGuillerme/dispRity)
[![Project Status: Active - The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)

## Help

If you need help with the package, hopefully the following manual will be useful.
However, parts of this package are still in development and some other parts are probably not covered.
Thus if you have suggestions or comments on on what has already been developed or will be developed, please send me an email (<guillert@tcd.ie>) or if you are a GitHub user, directly create an issue on the [GitHub page](https://github.com/TGuillerme/dispRity).

## Citations

You can cite both the package or this manual with the following citation:

> Guillerme, T. (2016). dispRity: a package for measuring disparity in R. Zenodo. 10.5281/zenodo.55646

<!-- TG: I'm gonna change that to a newest version with the submission -->

Note that this citation is only temporary (but can still be used!).
A future proper version of the latest package release, this manual and an associated methods paper should be submitted soon(ish!).

# Getting started

## What sort of data does dispRity work with?

Disparity must be measured from ordinated matrices.
These matrices can be from any type of ordination (PCO, PCA, PCoA, MDS, etc.) as long as they have your element names as rows (taxa, experiments, countries etc.) and your ordination axes as columns (the dimensions of your dataset).

### Ordination matrices from `Claddis`

```{r, eval = TRUE, echo = FALSE, message = FALSE}
## Checking if Claddis is installed BUILD ONLY
if(!require(Claddis)) install.packages("Claddis")
```

`dispRity` package can easily take data from `Claddis` using the `Claddis.ordination` function.
For this, simply input a matrix in the `Claddis` format to the function and it will automatically calculate and ordinate the distances among taxa:

```{r}
require(Claddis)

## Ordinating the example data from Claddis
Claddis.ordination(Michaux1989) 
```

Note that several options are available, namely which type of distance should be computed.
See more info in the function manual (`?Claddis.ordination`).
Alternatively, it is of course also possible to manual calculate the ordination matrix using the functions `Claddis::MorphDistMatrix` and `stats::cmdscale`.

### Ordination matrices from `geomorph`

```{r, eval = TRUE, echo = FALSE, message = FALSE}
## Checking if geomorph is installed BUILD ONLY
if(!require(geomorph)) install.packages("geomorph")
```

You can also easily use data from `geomorph` using the `geomorph.ordination` function.
This function simply takes Procrustes aligned data and performs an ordination:

```{r}
require(geomorph)

## Loading the plethodon dataset
data(plethodon)

## Performing a Procrustes transform on the landmarks
procrustes <- gpagen(plethodon$land, PrinAxes = FALSE, print.progress = FALSE)

## Ordinating this data
geomorph.ordination(procrustes)[1:5,1:5]
```

Options for the ordination (from `?prcomp`) can be directly passed to this function to perform customised ordinations.
Additionally you can give the function a `geomorph.data.frame` object.
If the latter contains sorting information (i.e. factors), they can be directly used to make a customised `dispRity` object [customised `dispRity` object](#customised-subsamples)!

```{r}
## Using a geomorph.data.frame
geomorph_df <- geomorph.data.frame(procrustes,
     species = plethodon$species, site = plethodon$site)

## Ordinating this data and making a dispRity object
geomorph.ordination(geomorph_df)
```

More about these `dispRity` objects below!

### Other kinds of ordination matrices

If you are not using the packages mentioned above (`Claddis` and `geomorph`) you can easily make your own ordination matrices by using the following functions from the `stats` package.
Here is how to do it for the following types of matrices:

 * Multivariate matrices (principal components analysis; PCA)

```{r}
## A multivariate matrix
head(USArrests)

## Ordinating the matrix using `prcomp` 
ordination <- prcomp(USArrests)

## Selecting the ordinated matrix
ordinated_matrix <- ordination$x
head(ordinated_matrix)
```
  This results in a ordinated matrix with US states as elements and four dimensions (PC 1 to 4). For an alternative method, see the `?princomp` function.

 * Distance matrices (classical multidimensional scaling; MDS)

```{r}
## A matrix of distances between cities
str(eurodist)

## Ordinating the matrix using cmdscale() with k = 5 dimensions 
ordinated_matrix <- cmdscale(eurodist, k = 5)
head(ordinated_matrix)
```

  This results in a ordinated matrix with European cities as elements and five dimensions.

Of course any other method for creating the ordination matrix is totally valid, you can also not use any ordination at all!
The only requirements for the `dispRity` functions is that the input is a matrix with elements as rows and dimensions as columns.

## Performing a simple dispRity analysis

Two `dispRity` functions allow users to run an analysis pipeline simply by inputting an ordination matrix.
These functions allow users to either calculate the disparity through time (`dispRity.through.time`) or the disparity of user-defined groups (`dispRity.per.group`).

### Example data

To illustrate these functions, we will use data from @beckancient2014.
This dataset contains an ordinated matrix of 50 discrete characters from mammals (`BeckLee_mat50`), another matrix of the same 50 mammals and the estimated discrete data characters of their descendants (thus 50 + 49 rows, `BeckLee_mat99`), a dataframe containing the ages of each taxon in the dataset (`BeckLee_ages`) and finally a phylogenetic tree with the relationships among the 50 mammals (`BeckLee_tree`).

```{r, fig.width=7, fig.height=7}
## Loading the ordinated matrices
data(BeckLee_mat50)
data(BeckLee_mat99)

## The first five taxa and dimensions of the 50 taxa matrix
head(BeckLee_mat50[, 1:5])

## The first five taxa and dimensions of the 99 taxa + ancestors matrix
BeckLee_mat99[c(1, 2, 98, 99), 1:5]

## Loading a list of first and last occurrence dates for the fossils
data(BeckLee_ages)
head(BeckLee_ages)

## Loading and plotting the phylogeny
data(BeckLee_tree)
plot(BeckLee_tree, cex = 0.8) 
axisPhylo(root = 140)
nodelabels(cex = 0.5)
```

<!-- I removed the semicolons above, it is confusing to novice R users -->

Of course you can use your own data as detailed in the [previous chapter](#which-data).
<!-- I changed the header so may have broken this link -->

### Disparity through time

The `dispRity.through.time` function calculates disparity through time, a common analysis in palaeontology.
This function (and the following one) uses an analysis pipeline with a lot of default parameters to make the analysis as simple as possible. 
Of course all the defaults can be changed if required, more on this later.

For a disparity through time analysis, you will need:
  
  * An ordinated matrix (we covered that above)
  * A phylogenetic tree: this must be a `phylo` object (from the `ape` package) and needs a `root.time` element. To give your tree a root time (i.e. an age for the root), you can simply do `my_tree$root.time <- my_age`.
  * The required number of time subsamples (here `time = 3`)
  * Your favourite disparity metric (here the sum of variances)

Using the @beckancient2014 data described [above](#example-data):

```{r}
##Â Measuring disparity through time
disparity_data <- dispRity.through.time(BeckLee_mat50, BeckLee_tree,
                                        time = 3, metric = c(sum, variances))
```

This generates a `dispRity` object (see [here](#manipulating-dispRity-objects) for technical details).
When displayed, these `dispRity` objects provide us with information on the operations done to the matrix:

```{r}
## Print the disparity_data object
disparity_data
```

We asked for three subsamples (evenly spread across the age of the tree), the data was bootstrapped 100 times (default) and the metric used was the sum of variances.

We can now summarise or plot the `disparity_data` object, or perform statistical tests on it (e.g. a simple `aov`):

```{r, fig.width=7, fig.height=7}
## Summarising disparity through time
summary(disparity_data)

## Plotting the results
plot(disparity_data, type = "continuous")

## Testing for an difference among the time bins
disp_aov <- test.dispRity(disparity_data, test = aov, comparisons = "all")
summary(disp_aov)

## Note that ANOVA assumptions were not checked here!
```

<!-- NC: Is this using aov or lm? Note that aov can only be used with balanced designs, so where the three time bins have equal numbers of species. I expect you need to use lm (which also fits an ANOVA as an ANOVA is also a linear model). THIS IS IMPORTANT -->

Please refer to the [specific tutorials](#specific-tutorial) for (much!) more information on the nuts and bolts of the package.
You can also directly explore the specific function help files within R and navigate to related functions.

### Disparity among groups

The `dispRity.per.group` function is used if you are interested in looking at disparity among groups rather than through time.
For example, you could ask if there is a difference in disparity between two groups?

To perform such an analysis, you will need:
 
 * An ordinated matrix (always!)
 * A list of group members: this list should be a list of numeric vectors or names corresponding to the row names in the matrix. For example `list("a" = c(1,2), "b" = c(3,4))` will create a group _a_ containing elements 1 and 2 from the matrix and a group _b_ containing elements 3 and 4. Note that elements can be present in multiple groups at once.
 * Your favourite disparity metric (here the sum of variances)

Using the @beckancient2014 data described [above](#example-data):

```{r}
## Creating the two groups (crown versus stem) as a list
mammal_groups <- list("crown" = c(16, 19:41, 45:50),
                      "stem" = c(1:15, 17:18, 42:44))

## Measuring disparity for each group
disparity_data <- dispRity.per.group(BeckLee_mat50, group = mammal_groups,
                                     metric = c(sum, variances))
```

We can display the disparity of both groups by simply looking at the output variable (`disparity_data`) and then summarising the `disparity_data` object and plotting it, and/or by performing a statistical test to compare disparity across the groups (here a Wilcoxon test).

```{r, fig.width=7, fig.height=7}
## Print the disparity_data object
disparity_data

## Summarising disparity in the different groups
summary(disparity_data)

## Plotting the results
plot(disparity_data)

## Testing for a difference between the groups
test.dispRity(disparity_data, test = wilcox.test, details = TRUE)
```

# Specific tutorials

The following section contains information specific to some functions.
If any of your questions are not covered in these sections, please refer to the function help files in R, send me an email (<guillert@tcd.ie>), or raise an issue on GitHub.
The several tutorials below describe specific functionalities of certain functions; please always refer to the function help files for the full function documentation!

## Time slicing

The function `time.subsamples` allows users to divide the ordinated matrix into different time subsamples or slices given a dated phylogeny that contains all the elements (i.e. taxa) from the matrix.
Each subsample generated by this function will then contain all the elements present at a specific point in time or during a specific period in time.

Two types of time subsamples can be performed by using the `method` option:

 *  discrete time subsamples (or time-binning) using `method = discrete`
 *  continuous time subsamples (or time-slicing) using `method = continuous`

For the time-slicing method details see @GuillermeSTD. 
<!-- NC: Or potentially this paper we are writing for the PalAss? -->
For both methods, the function takes the `time` argument which can be a vector of `numeric` values for:

 *  defining the boundaries of the time bins (when `method = discrete`)
 *  defining the time slices (when `method = continuous`)

Otherwise, the `time` argument can be set as a single `numeric` value for automatically generating a given number of equidistant time-bins/slices.
Additionally, it is also possible to input a dataframe containing the first and last occurrence data (FAD/LAD) for taxa that span over a longer time than the given tips/nodes age, so taxa can appear in more than one time bin/slice.

Here is an example for `method = discrete`:

```{r, eval=TRUE}
## Generating three time bins containing the taxa present every 40 Ma
time.subsamples(data = BeckLee_mat50, tree = BeckLee_tree, method = "discrete",
                time = c(120, 80, 40, 0))
```

Note that we can also generate equivalent results by just telling the function that we want three time-bins as follow:

```{r, eval=TRUE}
## Automatically generate three equal length bins:
time.subsamples(data = BeckLee_mat50, tree = BeckLee_tree, method = "discrete",
                time = 3)
```

In this example, the taxa were split inside each time-bin according to their age.
However, the taxa here are considered as single points in time.
It is totally possible that some taxa could have had longer longevity and that they exist in multiple time bins.
In this case, it is possible to include them in more than one bin by providing a table of first and last occurrence dates (FAD/LAD).
This table should have the taxa names as row names and two columns for respectively the first and last occurrence age:

```{r, eval=TRUE}
## Displaying the table of first and last occurrence dates for each taxa
head(BeckLee_ages)

## Generating time bins including taxa that might span between them
time.subsamples(data = BeckLee_mat50, tree = BeckLee_tree, method = "discrete",
                time = c(120, 80, 40, 0), FADLAD = BeckLee_ages)
```

When using this method, the oldest boundary of the first bin (or the first slice, see below) is automatically generated as the root age plus 1\% of the tree length, as long as at least three elements/taxa are present at that point in time.
The algorithm adds an extra 1\% tree length until reaching the required minimum of three elements.
It is also possible to include nodes in each bin by using `inc.nodes = TRUE` and providing a matrix that contains the ordinated distance among tips *and* nodes.

For the time-slicing method (`method = continuous`), the idea is fairly similar.
This option, however, requires a matrix that contains the ordinated distance among taxa *and* nodes and an extra argument describing the assumed evolutionary model (via the `model` argument).
This model argument is used when the time slice occurs along a branch of the tree rather than on a tip or a node, meaning that a decision must be made about what the value for the branch should be.
The model can be one of the following:

 *  `acctran` where the data chosen along the branch is always the one of the offspring
 *  `deltran` where the data chosen along the branch is always the one of the descendant
 *  `punctuated` where the data chosen along the branch is randomly chosen between the offspring or the descendant
 *  `gradual` where the data chosen along the branch is either the offspring or the descendant depending on branch length

<!--NC: Bit puzzled by the offspring/descendent terminology - I'd assume these were the same thing. By descendant do you mean ancestor? I'm confused  -->

```{r, eval=TRUE}
## Generating four time slices every 40 million years under a model of gradual evolution
time.subsamples(data = BeckLee_mat99, tree = BeckLee_tree, 
    method = "continuous", model = "gradual", time = c(120, 80, 40, 0),
    FADLAD = BeckLee_ages)

## Generating four time slices automatically
time.subsamples(data = BeckLee_mat99, tree = BeckLee_tree,
    method = "continuous", model = "gradual", time = 4, FADLAD = BeckLee_ages)
```

## Customised subsamples

Another way of separating elements into different categories is to use customised subsamples as briefly explained [above](#disparity-between-groups).
This function simply takes the list of elements to put in each group (whether they are the actual element names or their idea).

<!-- NC: Their idea? Huh? Whats that mean? -->

```{r, eval=TRUE}
## Creating the two groups as a list
mammal_groups <- list("crown" = c(16, 19:41, 45:50),
                      "stem" = c(1:15, 17:18, 42:44))

## Separating the dataset into two different groups
custom.subsamples(BeckLee_mat50, group = mammal_groups)
```

Elements can easily be assigned to different groups if necessary!

```{r, eval=FALSE}
## Creating the three groups as a list
mammal_groups <- list("crown" = c(16, 19:41, 45:50),
                      "stem" = c(1:15, 17:18, 42:44).
                      "all" = c(1:50))
```

## Bootstraps and rarefactions

One important step in analysing ordinated matrices is to pseudo-replicate the data to see how robust the results are, and how sensitive they are to outliers in the dataset.
This can be achieved using the function `boot.matrix` to bootstrap and/or rarefy the data.
The default options will bootstrap the matrix 100 times without rarefaction using the "full" bootstrap method (see below):

```{r, eval=TRUE}
## Default bootstrapping
boot.matrix(data = BeckLee_mat50)
```

The number of bootstrap replicates can be defined using the `bootstraps` option.
The method can be modified by controlling which bootstrap algorithm to use through the `boot.type` argument.
Currently two algorithms are implemented:

 * `full` where the bootstrapping is entirely stochastic (*n* elements are replaced by any *m* elements drawn from the data)
 * `single` where only one random element is replaced by one other random element for each pseudo-replicate

```{r, eval=TRUE}
## Bootstrapping with the single bootstrap method
boot.matrix(BeckLee_mat50, boot.type = "single")
```

This function also allows users to rarefy the data using the `rarefaction` argument.
Rarefaction allows users to limit the number of elements to be drawn at each bootstrap replication.
This is useful if, for example, one is interested in looking at the effect of reducing the number of elements on the results of an analysis.

This can be achieved by using the `rarefaction` option that draws only *n-x* at each bootstrap replicate (where *x* is the number of elements not sampled).
The default argument is `FALSE` but it can be set to `TRUE` to fully rarefy the data (i.e. remove *x* elements for the number of pseudo-replicates, where *x* varies from the maximum number of elements present in each subsample to a minimum of three elements).
It can also be set to one or more `numeric` values to only rarefy to the corresponding number of elements.

```{r, eval=TRUE}
## Bootstrapping with the full rarefaction
boot.matrix(BeckLee_mat50, bootstraps = 20, rarefaction = TRUE)

## Or with a set number of rarefaction levels
boot.matrix(BeckLee_mat50, bootstraps = 20, rarefaction = c(6:8, 3))
```

One additional important argument is `dimensions` that specifies how many dimensions from the matrix should be used for further analysis.
When missing, all dimensions from the ordinated matrix are used.

<!-- NC: This is confusing to me. If it's a number > 0, it keeps that number of dimensions, and if < 0 it removes the last dimensions by that %? This is very odd. Please explain -->

```{r, eval=TRUE}
## Removing the last 50% of the dimensions
boot.matrix(BeckLee_mat50, dimensions = 0.5)

## Keeping the first 10 dimensions
boot.matrix(BeckLee_mat50, dimensions = 10)
```

Of course, one could directly supply the subsamples generated above (using <!--  NC: which function ? How is this line different from the statement in the line below? -->) to this function.
It can also deal with a list of matrices or with a `dispRity` object output from the `custom.subsamples` or `time.subsamples` functions.

```{r, eval=TRUE}
## Creating subsamples of crown and stem mammals
crown_stem <- custom.subsamples(BeckLee_mat50,
                                group = list("crown" = c(16, 19:41, 45:50), 
                                             "stem" = c(1:15, 17:18, 42:44)))
## Bootstrapping and rarefying these groups
boot.matrix(crown_stem, bootstraps = 200, rarefaction = TRUE)


## Creating time slice subsamples
time_slices <- time.subsamples(data = BeckLee_mat99, tree = BeckLee_tree, 
                               method = "continuous", model = "gradual", 
                               time = c(120, 80, 40, 0),
                               FADLAD = BeckLee_ages)
## Bootstrapping the time slice subsamples
boot.matrix(time_slices, bootstraps = 100)
```

## Disparity metrics

There are many ways of measuring disparity!
In brief, disparity is a summary metric that will represent an aspect of an ordinated space (e.g. a MDS, PCA, PCO, PCoA).
For example, one can look at ellipsoid hyper-volume of the ordinated space @DonohueDim, the sum and the product of the ranges and variances @Wills1994 or the median position of the elements relative to their centroid @finlay2015morphological. 
<!-- NC: probably should cite Matt Wills' paper for the centroid thing too as he invented it apparently -->
Of course, there are many more examples of metrics one can use for describing some aspect of the ordinated space, with some performing better than other ones at particular descriptive tasks, and some being more generalist.

Because of this great diversity of metrics, the package `dispRity` does not have one way to measure disparity but rather proposes to facilitate users in defining their own disparity metric that will best suit their particular analysis.
In fact, the core function of the package, `dispRity`, allows the user to define any metric with the `metric` argument.
However the `metric` argument has to follow certain rules:

1.  It must be composed from one to three `function` objects;
2.  The function(s) must take as a first argument a `matrix` or a `vector`;
3.  The function(s) must be of one of the three dimension-levels described below;
4.  At least one of the functions must be of dimension-level 1 or 2 (see below).

<!-- NC: We discussed this previously, but I don't like the term levels here. It's confusing because it means something specific wrt factors. Can we change it to dimension-level? That's more descriptive and clearer in my opinion. I have changed it below, see what you think -->

### The function dimension-levels

The metric function dimension-levels determine the "dimensionality of decomposition" of the input matrix.
In other words, each dimension-level designates the dimensions of the output, i.e. either three (a `matrix`); two (a `vector`); or one (a single `numeric` value) dimension.

![Illustration of the different dimension-levels of functions with an input `matrix`](dispRity_fun.png)

#### Dimension-level 1 functions

A dimension-level 1 function will decompose a `matrix` or a `vector` into a single value:

```{r}
## Creating a dummy matrix
dummy_matrix <- matrix(rnorm(12), 4, 3)

## Example of dimension-level 1 functions
mean(dummy_matrix)
median(dummy_matrix)
```

Any summary metric such as mean or median are good examples of dimension-level 1 functions as they reduce the matrix to a single dimension (i.e. one value).

#### Dimension-level 2 functions

A dimension-level 2 function will decompose a `matrix` into a `vector`.

```{r}
## Defining the function as the product of rows
prod.rows <- function(matrix) apply(matrix, 1, prod)

## A dimension-level 2 metric
prod.rows(dummy_matrix)
```

Several dimension-level 2 functions are implemented in `dispRity` (see `?dispRity.metric`) such as the `variances` or `ranges` functions that calculate the variance or the range of each dimension of the ordinated matrix respectively.

#### Dimension-level 3 functions

Finally a dimension-level 3 function will transform the matrix into another matrix.
Note that the dimension of the output matrix doesn't need to match the the input matrix:

```{r}
## A dimension-level 3 metric
var(dummy_matrix)

## A dimension-level 3 metric with a forced matrix output
as.matrix(dist(dummy_matrix))
```

### `make.metric`

Of course, functions can be more complex and involve multiple operations such as the `centroids` function (see `?dispRity.metric`) that calculates the Euclidean distance between each element and the centroid of the ordinated space.
The `make.metric` function implemented in `dispRity` is designed to help test and find the dimension-level of the functions.
This function tests:

1.  If your function can deal with a `matrix` or a `vector` as an input;
2.  Your function's dimension-level according to its output (dimension-level 1, 2 or 3, see above);
3.  Whether the function can be implemented in the `dispRity` function (the function is fed into a `lapply` loop).

For example, let's see if the functions described above are the right dimension-levels:

```{r}
## Which dimension-level is the mean function? And can it be used in dispRity?
make.metric(mean)

## Which dimension-level is the prod.rows function? And can it be used in dispRity?
make.metric(prod.rows)

## Which dimension-level is the var function? And can it be used in dispRity?
make.metric(var)
```

A non verbose version of the function is also available.
This can be done using the option `silent = TRUE` and will simply output the dimension-level of the metric.

<!-- NC: My terminology changes may have broken this bit -->

```{r}
## Testing whether mean is dimension-level 1
if(make.metric(mean, silent = TRUE) != "dimension-level1") {
    message("The metric is not dimension-level 1.")
}
## Testing whether var is dimension-level 1
if(make.metric(var, silent = TRUE) != "dimension-level1") {
    message("The metric is not dimension-level 1.")
}
```

### Metrics in the `dispRity` function

Using this metric structure, we can easily use any disparity metric in the `dispRity` function as follows:

```{r}
## Measuring disparity as the standard deviation of all the values of the
## ordinated matrix (dimension-level 1 function).
summary(dispRity(BeckLee_mat50, metric = sd))

## Measuring disparity as the standard deviation of the variance of each axis of
## the ordinated matrix (dimension-level 1 and 2 functions).
summary(dispRity(BeckLee_mat50, metric = c(sd, variances)))

## Measuring disparity as the standard deviation of the variance of each axis of
## the variance covariance matrix (dimension-level 1, 2 and 3 functions).
summary(dispRity(BeckLee_mat50, metric = c(sd, variances, var)), round = 10)
```

Note that the order of each function in the metric argument does not matter, the `dispRity` function will automatically detect the function dimension-levels (using `make.metric`) and apply them to the data in decreasing order (dimension-level 3 > dimension-level 2 > dimension-level 1).

```{r}
## Disparity as the standard deviation of the variance of each axis of the
## variance covariance matrix:
disparity1 <- summary(dispRity(BeckLee_mat50, metric = c(sd, variances, var)),
                      round = 10)

## Same as above but using a different function order for the metric argument
disparity2 <- summary(dispRity(BeckLee_mat50, metric = c(variances, sd, var)),
                      round = 10)

## Both ways output the same disparity values:
disparity1 == disparity2
```

In these examples, we considered disparity to be a single value.
For example, in the previous example, we defined disparity as the standard deviation of the variances of each column of the variance/covariance matrix (`metric = c(variances, sd, var)`).
It is, however, possible to calculate [disparity as a distribution](#disparity-as-a-distribution).

### Metrics implemented in `dispRity`

Several disparity metrics are implemented in the `dispRity` package.
The detailed list can be found in `?dispRity.metric` along with some description of each metric.

Level | Name | Description | Source | 
------|------|---------------------------------------------------|--------|
3 | `ellipse.volume`<sup>1</sup> | The volume of the ellipsoid of the space | [@DonohueDim] |
3 | `convhull.surface` | The surface of the convex hull formed by all the elements | [`geometry`](https://cran.r-project.org/web/packages/geometry/index.html)`::convhulln` |
3 | `convhull.volume` | The volume of the convex hull formed by all the elements | [`geometry`](https://cran.r-project.org/web/packages/geometry/index.html)`::convhulln` |
3 | `hypervolume` | The volume of the ordinated space | [`hypervolume`](https://cran.r-project.org/web/packages/hypervolume/index.html)`::hypervolume` |
3 | `diagonal` | The longest distance in the ordinated space (like the diagonal in two dimensions) | `dispRity` |
2 | `ranges` | The range of each dimension | `dispRity` |
2 | `variances` | The variance of each dimension | `dispRity` |
2 | `centroids`<sup>2</sup> | The distance between each element and the centroid of the ordinated space | `dispRity` |
1 | `mode.val` | The modal value | `dispRity` |


1: This function uses an estimation of the eigenvalue that only works for MDS or PCoA ordinations (not PCA).

2: Note that by default, the centroid is the centroid of the elements.
It can, however, be fixed to a different value by using the `centroid` argument `centroids(space, centroid = rep(0, ncol(space)))`, for example the origin of the ordinated space.

### Equations and implementations
Some of the functions described below are implemented in the `dispRity` package and do not require any other packages to calculate ([see implementation here](https://github.com/TGuillerme/dispRity/blob/master/R/dispRity.metric.R)).

\begin{equation}
    ellipse.volume = \frac{\pi^{k/2}}{\Gamma(\frac{k}{2}+1)}\displaystyle\prod_{i=1}^{k} (\lambda_{i}^{0.5})
\end{equation}
Where *k* is the number of dimensions, and $\lambda_i$ is the eigenvalue of each dimension.

\begin{equation}
    diagonal = \sqrt{\sum_{i=1}^{k}|max(k_i) - min(k_i)|}
\end{equation}
Where *k* is the number of dimensions.

\begin{equation}
    ranges = |max(k_i) - min(k_i)|
\end{equation}
Where *k* is the number of dimensions.

\begin{equation}
    variances = \sigma^{2}{k_i}
\end{equation}
Where *k* is the number of dimensions, and $\sigma^{2}$ is their variance.

\begin{equation}
    centroids = \sqrt{\sum_{i=1}^{n}{({k}_{n}-Centroid_{k})^2}}
\end{equation}
Where *n* is each element in the ordinated space, *k* is the number of dimensions, and $Centroid_{k}$ is their mean (or can be set to another value).

### Metrics run through
Here is a run through of the main metrics implemented in `dispRity`.
First, we will create a dummy ordinated space using the `space.maker` utility function (more about that [here](#space.maker):

```{r}
## Creating a 10*5 normal space
set.seed(1)
dummy_space <- space.maker(10, 5, rnorm)
```

#### Volumes and surface metrics
The functions `ellipse.volume`, `convhull.surface`, `convhull.volume` and `hyper.volume` all measure the surface or the volume of the ordinated space occupancy:

```{r}
## Calculating the ellipsoid volume
summary(dispRity(dummy_space, metric = ellipse.volume))
```
> Because there is only one subsamples (i.e. one matrix) in the dispRity object, this operation is the equivalent of doing simply `ellipse.volume(dummy_space)` (with rounding).

```{r}
## Calculating the convex hull surface
summary(dispRity(dummy_space, metric = convhull.surface))

## Calculating the convex hull volume
summary(dispRity(dummy_space, metric = convhull.volume))
```

The convex hull functions make a (good) estimation of the multidimensional properties of the ordinated space.
For the full and correct calculation of the volume of the ordinated space, it is possible to us the `hyper.volume` function that intakes more options but takes longer to calculate.

```{r}
## Calculating the true multidimensional volume
summary(dispRity(dummy_space, metric = hyper.volume))

## The volume with different options to be passed to hypervolume::hypervolume
summary(dispRity(dummy_space, metric = hyper.volume, repsperpoint = 100,
                 quantile = 0.5))
```
> Cautionary note: measuring volumes in a high number of dimensions can be strongly affected by the [Curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) which often results in near 0 disparity values.

#### Ranges, variances and diagonal

The functions `ranges`, `variances` and `diagonal` all measure properties of the ordinated space base on it's dimensions properties (they are also less affected by the Curse of dimensionality):

`ranges` and `variances` both work on the same principle and measure the range/variance of each dimensions:

```{r}
## Calculating the ranges of each dimensions in the ordinated space
ranges(dummy_space)

## Calculating disparity as the distribution of these ranges
summary(dispRity(dummy_space, metric = ranges))

## Calculating disparity as the sum and the product of these ranges
summary(dispRity(dummy_space, metric = c(sum, ranges)))
summary(dispRity(dummy_space, metric = c(prod, ranges)))

## Calculating the variances of each dimensions in the ordinated space
variances(dummy_space)

## Calculating disparity as the distribution of these variances
summary(dispRity(dummy_space, metric = variances))

## Calculating disparity as the sum and the product of these variances
summary(dispRity(dummy_space, metric = c(sum, variances)))
summary(dispRity(dummy_space, metric = c(prod, variances)))
```

The `diagonal` function measures the multidimensional diagonal of the whole space (i.e. in our case the longest euclidean distance in our 5 dimensional space):


```{r}
## Calculating the ordinated space's diagonal
summary(dispRity(dummy_space, metric = diagonal))
```

> This metric is only an euclidean diagonal (mathematically valid) if the dimensions within the space are all orthogonal!

#### Centroids metrics

The `centroids` metric allows to measure the position of the different elements compared to a fixed point in the ordinated space.
By default, this function measures the distance between each elements and the central point between them:

```{r}
## The distribution of the distances between each elements and the centroid
summary(dispRity(dummy_space, metric = centroids))

## Disparity as the median value of these distances
summary(dispRity(dummy_space, metric = c(median, centroids)))
```

It is however possible to fix the coordinates of a specific point in the ordinated space as long as it has the correct number of dimensions:

```{r}
## The distance between the elements and the origin of the ordinated space
summary(dispRity(dummy_space, metric = centroids, centroid = c(0,0,0,0,0)))

## Disparity as the distance between the elements and a specific point in space
summary(dispRity(dummy_space, metric = centroids, centroid = c(0,1,2,3,4)))
```


## Summarising dispRity data (plots)

Because of its architecture, printing `dispRity` objects only summarise their content but does not print the disparity value measured or associated analysis (more about this [here](#manipulating-dispRity-objects)).
To actually summary what's in a dispRity object, one can either use the `summary` function for visualising the data in a table or `plot` to have a graphic representation of the results.


### Summarising `dispRity` data
This function is a S3 function (`summary.dispRity`) allowing to summarise the content of `dispRity` objects that contain disparity calculations.


<!-- DATA LOADER -->


```{r}
## Example data from previous sections
crown_stem <- custom.subsamples(BeckLee_mat50,
    group = list("crown" = c(16, 19:41, 45:50), "stem" = c(1:15, 17:18, 42:44)))
## Bootstrapping and rarefying these groups
boot_crown_stem <- boot.matrix(crown_stem, bootstraps = 100, rarefaction = TRUE)
## Calculate disparity
disparity_crown_stem <- dispRity(boot_crown_stem, metric = c(sum, variances))

## Creating time slices subsamples
time_slices <- time.subsamples(data = BeckLee_mat99, tree = BeckLee_tree, 
    method = "continuous", model = "gradual", time = c(120, 80, 40, 0),
    FADLAD = BeckLee_ages)
## Bootstrapping on the time binning/slicing subsamples
boot_time_slices <- boot.matrix(time_slices, bootstraps = 100)
## Calculate disparity
disparity_time_slices <- dispRity(boot_time_slices, metric = c(sum, variances))

## Creating time bins subsamples
time_bins <- time.subsamples(data = BeckLee_mat99, tree = BeckLee_tree, 
    method = "discrete", time = c(120, 80, 40, 0), FADLAD = BeckLee_ages,
    inc.nodes = TRUE)
## Bootstrapping on the time binning/slicing subsamples
boot_time_bins <- boot.matrix(time_bins, bootstraps = 100)
## Calculate disparity
disparity_time_bins <- dispRity(boot_time_bins, metric = c(sum, variances))
```

This objects are easy to summarise as follow:

```{r}
## Default summary
summary(disparity_time_slices)
```

Information about the number of elements per subsamples as well as the observed (i.e. non-bootstrapped) disparity is also calculated.
This is specifically handy when rarefying the data for example

```{r}
head(summary(disparity_crown_stem))
```

The summary functions can also take various options such as:

 * the `quantile` values for the confidence intervals levels (by default, the 50 and 95 quantiles are calculated)
 * the `cent.tend` for the  central tendency to use for summarising the results (default is `median`)
 * the `rounding` option corresponding to the number of decimals to print (default is `2`)
 *  and the `recall` option for printing the call of the `dispRity` object as well (default is `FALSE`)

These options can easily be changed from the defaults as follows:

```{r}
## Same but using the 88 quantile and the standard deviation as the summary. 
summary(disparity_time_slices, quantile = 88, cent.tend = sd)

## Printing the details of the object and rounding the values to the 5th decimal
summary(disparity_time_slices, recall = TRUE, rounding = 5)
```

Information about the number of elements per subsamples as well as the observed (i.e. non-bootstrapped) disparity is also calculated.
This is specifically handy when rarefying the data for example

Note that the summary table is a `data.frame`, hence it is easy to modify as any table using `dplyr`.
You can also export it in `csv` format using `write.csv` or even directly export into `LaTeX` format using the following;

```{r, eval=FALSE}
## Loading the xtable package
require(xtable)
## Converting the table in LaTeX
xtable(summary(disparity_time_slices))
```

### Plotting `dispRity` data
An alternative (and more fun!) way to display the calculated disparity is to plot the results using the S3 method `plot.dispRity`.
This function intakes the same options as `summary.dispRity` along side with various graphical options described in the function manual (see `?plot.dispRity`).

The plots can be of four different types:
 
 * `continuous` for displaying continuous disparity curves
 * and `box`, `lines` and `polygons` to display discrete disparity results in respectively a boxplot, confidence interval lines and confidence interval polygons.

> This argument can be left missing. In this case, the algorithm will automatically detect the type of subsamples from the `dispRity` object.

It is also possible to display the number of elements per subsamples (has a horizontal doted line) using the option `elements = TRUE`.
Additionally, when the data is rarefied, one can also indicate which level of rarefaction to display (i.e. display the results for a certain number of elements) by using the `rarefaction` argument.

```{r, fig.width=8, fig.height=8}
## Graphical parameters
op <- par(mfrow = c(2,2), bty = "n")

## Plotting continuous disparity results
plot(disparity_time_slices, type = "continuous")

## Plotting discrete disparity results
plot(disparity_crown_stem, type = "box")

## Same but using lines for the rarefaction level of 20 elements only
plot(disparity_crown_stem, type = "line", rarefaction = 20)

## Same but using polygons while also displaying the number of elements
plot(disparity_crown_stem, type = "polygon", elements = TRUE)

## Resetting graphical parameters
par(op)
```

Since `plot.dispRity` uses the arguments from the generic `plot` method, it is of course possible to change pretty much everything using the regular plot arguments:

```{r, fig.width=8, fig.height=8}
## Graphical options
op <- par(bty = "n")

## Plotting the results with some classic options from plot
plot(disparity_time_slices, col = c("blue", "orange", "green"),
    ylab = c("Some measurement"), xlab = "Some other measurement",
    main = "Many options...", ylim = c(5, 0), xlim = c(4, 0))

## Adding a legend
legend("topleft", legend = c("Central tendency",
                             "Confidence interval 1",
                            "Confidence interval 2"),
     col = c("blue", "orange", "green"), pch = 19)

## Resetting graphical parameters
par(op)
```

Additionally to the classic `plot` arguments, the function can also take arguments that are specific to `plot.dispRity` like adding the number of elements or rarefaction level (as described above) but also changing the values of the quantiles to plot as well as the central tendency.

```{r, fig.width=8, fig.height=8}
## Graphical options
op <- par(bty = "n")

## Plotting the results with some plot.dispRity arguments
plot(disparity_time_slices, quantile = c(seq(from=10, to=100, by=10)),
    cent.tend = sd, type = "c", elements = TRUE, col = c("black", rainbow(10)),
    ylab = c("Disparity", "Diversity"), time.subsamples = FALSE,
    xlab = "Time (in in units from past to present)", observed = TRUE,
    main = "Many more options...")

## Resetting graphical parameters
par(op)
```

> Note that the argument `observed = TRUE` allows to plot the disparity values calculated from the non-bootstrapped data as crosses on the plot.

For comparing results, it is also possible to add a plot to the existent plot by using `add = TRUE`:

```{r, fig.width=8, fig.height=8}
## Graphical options
op <- par(bty = "n")

## Plotting the continuous disparity with a fixed y axis
plot(disparity_time_slices, ylim = c(1, 4))
## Adding the discrete data
plot(disparity_time_bins, type = "line", ylim = c(1, 4), xlab = "", ylab = "",
    add = TRUE)

## Resetting graphical parameters
par(op)
```

Finally, if your data has been fully rarefied, it is also possible to easily look at rarefaction curves by using the `rarefaction = TRUE` argument:

```{r, fig.width=8, fig.height=8}
## Graphical options
op <- par(bty = "n")

## Plotting the rarefaction curves
plot(disparity_crown_stem, rarefaction = TRUE)

## Resetting graphical parameters
par(op)
```

## Testing disparity hypothesis

The `dispRity` package allows to apply some test functions to the calculated disparity in order to test hypothesis.
The function `test.dispRity` works in a similar way as the `dispRity` function: it intakes a `dispRity` object, a `test` and a `comparisons` argument.

The `comparisons` argument must indicate the way the test should be applied to the data:

 * `pairwise` (default): to compare each subsamples pairwise
 * `referential`: to compare each subsamples to the first one
 * `sequential`: to compare each subsamples to the following one
 * `all`: to compare all the subsamples together (like in analysis of variance)

It is also possible to input a list of pairs of `numeric` values or `characters` matching the subsamples names to create personalised test.
Some other specific tests implemented in `dispRity` such as the `dispRity::null.test` that has a specific way to be applied to the data and therefore ignore the `comparisons` argument. 
<!-- Add sequential test one day! -->

The `test` argument can be any statistical or non-statistical test to apply to the disparity object.
It can be common statistical test functions (e.g. `stats::t.test`), some functions implemented in `dispRity` (e.g. see `?null.test`) or any function defined by the user.

This function also allows to correct for type I error inflation when using multiple comparisons via the `correction` argument. This argument can be empty (no correction applied) or can contain one of the corrections from the `stats::p.adjust` function (see `?p.adjust`).

Note that the `test.dispRity` algorithm deals with some classic tests outputs (`h.test`, `lm` and `numeric` vector) and summarises the test output.
It is however possible to get the full detailed output by using the options `details = TRUE`.

```{r}
## T-test to the test a difference in disparity between crown and stem mammals
test.dispRity(disparity_crown_stem, test = t.test)

## Performing the same test but with the detailed t.test output
test.dispRity(disparity_crown_stem, test = t.test, details = TRUE)

## Wilcoxon test applied to sliced disparity the data sequentially with correction
test.dispRity(disparity_time_slices, test = wilcox.test,
              comparisons = "sequential", correction = "bonferroni")

## Measuring the overlap between distributions in the time bins (using the
## implemented Bhattacharyya Coefficient function - see ?bhatt.coeff)
test.dispRity(disparity_time_bins, test = bhatt.coeff)
```

It is also possible to apply some more *complex* tests that have their own output class (like `stats::aov`).
The results can then be analysed as usual using the associated `summary` S3 method:

```{r}
## Performing and ANOVA applied to the same data
(slice_aov <- test.dispRity(disparity_time_slices, test = aov,
                            comparisons = "all"))

## The output is a regular ANOVA output
class(slice_aov)

## That can be summarised using summary
summary(slice_aov)
```

Of course, following the modular design of the package, tests can always be made by the user (the same way disparity metrics can be user made).
The only condition is that the test can be applied to at least two distributions.
In practice, the `test.dispRity` function will pass the calculated disparity data (distributions) to the provided function in either pairs of distributions (if the `comparisons` argument is set to `pairwise`, `referential` or `sequential`) or a table containing all the distributions (`comparisons = all`; this should be in the same format as data passed to `aov` for example).

## Disparity as a distribution

Disparity is often regarded as a summary value of the position of the all elements in the ordinated space.
For example, the sum of variances, the product of ranges or the median distance between the elements and their centroid will summarise disparity as a single value.
This value can be of course pseudo-replicated (bootstrapped) in order to obtain a more confident distribution of this summary metric.
However, another way to perform disparity analysis is to use the whole distribution rather than their summary (e.g. the variances or the ranges).

This is possible in the `dispRity` package by calculating disparity as a dimension-level 2 metric only!
Let's have a look using our previous example of bootstrapped time slices but by measuring the distances between each taxa and their centroid as disparity

```{r}
## Measuring disparity as a whole distribution
disparity_centroids <- dispRity(boot_time_slices, metric = centroids)
```

The resulting disparity object being a dimension-level 2, it can easily also be transformed into a dimension-level 1 one by for example measuring the median distance of all these distributions:

```{r}
## Measuring median disparity in each time slice
disparity_centroids_median <- dispRity(disparity_centroids, metric = median)
```

And we can now compare the differences between both methods:

```{r}
## Summarising both disparity measurements:
## The distributions:
summary(disparity_centroids)
## The summary of the distributions (as median)
summary(disparity_centroids_median)
```

We can see that the summary message for the distribution is slightly different than before.
Here `summary` also displays the observed central tendency (i.e. the central tendency of the measured distributions).
Note that, as expected, this central tendency is the same in both metrics!

Another, maybe more intuitive way, to compare both approaches for measuring disparity is to plot the distributions:

```{r, fig.width=8, fig.height=8}
## Graphical parameters
op <- par(bty = "n", mfrow = c(1,2))

## Plotting both disparity measurements
plot(disparity_centroids, ylab = "Distribution of all the distances")
plot(disparity_centroids_median,
     ylab = "Distribution of the medians of all the distances")

par(op)
```

The resulted distributions can then be studied the same way as described above.


```{r}
## Probability of overlap in the distribution of medians
test.dispRity(disparity_centroids_median, test = bhatt.coeff)
```

In this case, we are looking at the probability of overlap of the distribution of median distances from centroids between each pair of time slices.
In other words, we are measuring whether the medians from each bootstrap pseudo-replicate for each time slice overlap.
But of course, we might be interested in the actual distribution of the distances from the centroid rather than simply their central tendencies.
This can be problematic depending on the research question asked since we are effectively comparing non-independent medians distributions (because of the pseudo-replication).

One solution is thus to look at the full distribution:


```{r}
## Probability of overlap for the full distributions
test.dispRity(disparity_centroids, test = bhatt.coeff)
```

These results show the actual overlap between all the measured distances from centroids concatenated across all the bootstraps.
For example, when comparing the slices 120 and 80, we are effectively comparing the 5 \times 100 distances (the distances of the 5 elements in slice 120 bootstrapped 100 times) to the 19 \times 100 distances from slice 80.
However, this can also be problematic for some specific tests since the _n_ \times 100 distances are also pseudo-replicates and thus are still not independent.

One second solution is to compare the distributions to each other _per replicates_:

```{r}
## Boostrapped probability of overlap for the full distributions
test.dispRity(disparity_centroids, test = bhatt.coeff, concatenate = FALSE)
```

These results show the median overlap between pairs of distributions in the first column (`bhatt.coeff`) and then the distribution of these overlaps between each par of bootstraps.
In other words, when two distributions are compared, they are now compared for each bootstrap pseudo-replicate, thus effectively creating a distribution of probabilities of overlap.
For example, when comparing the slices 120 and 80, we have a mean probability of overlap of 0.28 and a probability between 0.18 and 0.43 in 50% of the pseudo-replicates.
Note that the quantiles and central tendencies can be modified via the `conc.quantiles` option:

## Ecological study

Disparity analysis are not restricted to palaeobiology analysis only.
This package is designed for dealing with ordinated matrix, no matter which data was used in the ordination.
Here is an short example of disparity analysis using ecological data from D. McClean (unpubl.).

The dataset contains an ordinated matrix of 20 dimensions (columns) for 40 elements (rows).
The elements are different experimental plots with discrete variations of nutrient enrichment and depth and freshwater benthic invertebrates.

```{r}
## Loading the data
data(McClean_data)

## Ordinated matrix
ordinated_matrix <- McClean_data$ordination

## Treatments and depths
treatments <- McClean_data$treatment
depth <- McClean_data$depth
```

### Classic analysis

A classical way to represent this ordinated data will be to use two dimensional plots.

```{r, fig.width=6, fig.height=6}
## Setting the colors that will represent the treatments
cols <- sub("a", "red", treatments)
cols <- sub("b", "blue", cols)
## Setting the symbols that will represent the depth
pchs <- sub(1, 16, depth)
pchs <- as.numeric(sub(2, 17, pchs))

## Graphical option
op <- par(bty = "n")
## A classic 2D ordination plot
plot(ordinated_matrix[, 1:2], col = cols, pch = pchs,
     xlab = "PC 1", ylab = "PC 2",
     xlim = range(ordinated_matrix[, 1:2]) + c(0, 100))
par(op)
```

This shows the distribution of the experimental plots along the two first axis of variance of the ordinated distance matrix (i.e. the first two dimensions).
At a first glance, it seems difficult to see a clear effect from the treatments (blue or red) or the depth (rounds or triangles) since the different experimental plots with the same parameters donât seem to cluster.
However, the problem, is that this plot ignores the 18 other dimensions of the ordination!
Additionally, these two represented dimensions do not represent a biological reality *per se*; i.e. the values on the first dimension do not represent some continuous traits (e.g. depth) but just the ordinations of correlations between the data and some factors.

Therefore, we might want to approach this problem without getting stuck in only two dimensions and consider the whole dataset as a *n*-dimensional object.

### Multidimensional analysis with `dispRity`

#### Splitting the data following the different factors

The first step in such analysis will be to create different subsamples that will represent sub-samples of the ordinated space (i.e. sub-regions within the *n*-dimensional object).
Each of these subsamples contain a certain number of elements (i.e. a subset of the 40 experimental field plots) that have some attributes in common.
In our example, we are going to group the elements according to their depth and treatment.

```{r}
## Creating the table that contain the elements and their attributes
factors <- as.data.frame(matrix(data = c(treatments, depth),
          nrow = nrow(ordinated_matrix), ncol = 2, byrow = FALSE,
          dimnames = list(rownames(ordinated_matrix))))
names(factors) <- c("Treat", "Depth")
head(factors)
```

Second, letâs split the data according to these factors to create the subsamples of the ordinated space by using the `custom.subsamples` function:

```{r}
## Splitting the ordinated space into four subsamples
(customised_subsamples <- custom.subsamples(ordinated_matrix, factors))
```

#### Calculating disparity

In this example, we are going to define disparity as the n-dimensional ellipsoid volume (see `?hyper.volume`).

```{r}
## Calculating disparity as the ellipsoid volume of each group
disparity <- dispRity(customised_subsamples, metric = diagonal)
summary(disparity)
```


We can also want to bootstrap the data to test the robustness of the measured disparity to outliers.
Also, as we can see, each subsamples has different numbers of elements.
It might also be interesting to rarefy the data as well to have only subsamples with the same number of elements.

```{r}
## Bootstrapping the subsamples and using rarefaction
## (i.e. only re-sampling 17 elements each time)
bootstrapped_data <- boot.matrix(customised_subsamples, bootstraps = 100,
                                 rarefaction = 17)
```

We can now rerun a more robust disparity analysis using the bootstrapped data:

```{r}
## Calculating the bootstrapped disparity
disparity <- dispRity(bootstrapped_data, metric = diagonal)
```

#### Summarising and displaying the data

Now let's summarise this data:

```{r, fig.width=12, fig.height=6, eval=TRUE}
##Â Summary table of the data
summary(disparity)

## Graphical options
op <- par(mfrow = (c(1,2)), bty = "n")

## Plotting the bootstrapped disparity
plot(disparity, main = "Bootstrapped", las = 2, xlab = "")

## Plotting the rarefied disparity
plot(disparity, rarefaction = 17, main = "Rarefied", las = 2, xlab = "")

par(op)
```

As we can see, there seems to be no strong effect of the number of experimental plots in each subsamples (i.e. the rarefied plot is really similar to the bootstrapped plot) which is a good thing!

#### Summarising hypothesis

Finally, we can test our hypothesis (whether the sort of water treatment at certain depth alters invertebrate communities and composition in natural habitats) by using the `test.dispRity` function.

```{r}
## Testing the effect of our factors on the bootstrapped data
summary(test.dispRity(disparity, test = aov, comparisons = "all"))
```


## Simulating discrete morphological data

The `dispRity` package also allows to simulate discrete morphological data matrices.

### Quick go through
The function basically intakes a phylogenetic tree, the number of required characters and the evolutionary model and a function from which to draw the rates.
The package then proposes a function for quickly checking the matrix' phylogenetic signal using parsimony.

```{r}
set.seed(3)
## Setting the starting tree with 15 taxa as a random coalescent tree
my_tree <- rcoal(15)

## Generating a matrix with 100 characters (85% binary and 15% three states) and
## an equal rates model with a gamma rates distribution (0.5, 1) with no 
## invariant characters.
my_matrix <- sim.morpho(tree = my_tree, characters = 100, states = c(0.85,
    0.15), rates = c(rgamma, 0.5, 1), invariant = FALSE)

## The head of the matrix
my_matrix[1:5, 1:10]

## Checking the matrix properties with a quick Maximum Parsimony tree search
check.morpho(my_matrix, my_tree)
```

Note that this example produces a tree with a great consistency index and an identical topology as the random coalescent tree!
Nearly too good to be true...

### In details

Basically, `sim.morpho` is really flexible and intakes a lot of different arguments to allow to simulate realistic matrix.
It has three implemented models: `"ER"` for Equal Rates (the M*k* model); the `"HKY"` one which is the molecular HKY model but transforms pyrines in 0's and purimidines in 1's; or the `"mixed"` model that randomly uses an `"ER"` or and `"HKY"` for the binary characters and `"ER"` for the multistates (>2) characters.
Both models intakes specific distributions for their rates or substitution models.
These distributions should be passed to these arguments in the format of `c(sampler_function, distribution_parameters)` where the the sampler function is a the random generation function of that distribution (e.g. `rnorm`, `runif`, etc...) and the parameters are any parameters to be passed to this function. 

The `check.morpho` runs a quick maximum parsimony tree using the `phangorn` parsimony algorithm.
It quickly calculates the parsimony score, the consistency and retention indices and, if a tree is provided (e.g. the tree used to generate the matrix) it calculates the Robinson-Foulds distance between the most parsimonious tree and the provided tree.

### Parameters for a good(ish) matrix
There are many parameters that can realise a ``realistic'' matrix (meaning not to different from the input tree with a consistency and retention index close to what's in the literature) but because of the randomness of the generation (the code generates random matrices after all!) not all end up in creating good matrices.
The following parameters however, seems to generate a fairly ``realistic'' with a starting coalescent tree, equal rates model with 0.85 binary characters and 0.15 three states characters, a gamma distribution with a shape parameter ($\alpha$) of 5 and no scaling ($\beta$ = 1) with a rate of 100.

```{r}
set.seed(0)
## tree
my_tree <- rcoal(15)
## matrix
morpho_mat <- sim.morpho(my_tree, characters = 100, model = "ER",
    rates = c(rgamma, rate = 100, shape = 5), invariant = FALSE)
check.morpho(morpho_mat, my_tree)
```

### `space.maker`
Another way to simulate data is to directly simulate an ordinated space with the `space.maker` function.
This function allows to simulated multidimensional spaces given a certain number of properties.
It takes as arguments the number of elements (data points)Â and dimensions to create the space but also allows more fine tuning data simulation: it is possible to give a specific distribution to each dimensions, provide a correlation matrix to link the dimensions or even providing an *a priori* distribution of the variance per distributions!


```{r, fig.width=8, fig.height=8, eval=TRUE}
## Graphical options
op <- par(mfrow = (c(2,2)), bty = "n")
## Visualising 3D spaces
require(scatterplot3d)

## A cube space
scatterplot3d(space.maker(2500, 3, runif), pch = 20,
              xlab = "", ylab = "", zlab = "",
              main = "Uniform cube space")

## A plane space
scatterplot3d(space.maker(2500, 3, c(runif, runif, runif),
              arguments = list(list(min = 0, max = 0), NULL, NULL)), pch = 20,
              xlab = "", ylab = "", zlab = "",
              main = "Uniform plane space")

## An ellipsoid space (=a spheric space with correlation)
cor_matrix <- matrix(cbind(1,0.8,0.2, 0.8,1,0.7, 0.2,0.7,1), nrow = 3)
scatterplot3d(space.maker(2500, 3, rnorm, cor.matrix = cor_matrix), pch = 20,
              xlab = "", ylab = "", zlab = "",
              main = "Normal ellipsoid space")

## A cylindrical space with a decreasing variance per axis
scatterplot3d(space.maker(2500, 3, c(rnorm, rnorm, runif),
              scree = c(0.7, 0.2, 0.1)), pch = 20,
              xlab = "", ylab = "", zlab = "",
              main = "Normal cylindrical space")

## Resetting the graphic parameters
par(op)
```



# Glossary <a name="glossary"></a>

-   **Ordinated space**. The mathematical multidimensional object that will be analysed with this package.
    In morphometrics, this is often referred to as the morphospace.
    However it may also be referred to as the cladisto-space for cladistic data or the eco-space for ecological data etc.
    In practice, this term designates an ordinated matrix where the columns represent the dimensions of the space (often â but not necessarily - > 3!) and the rows represent the elements within this space.

-   **Elements**. The rows of the ordinated space. Elements can be taxa, field sites, countries etc.

-   **Dimensions**. The columns of the ordinated space. The dimensions are referred to as axes of variation, or principal components, for ordinated spaces obtained from a PCA.

-   **Subsamples**. Sub-samples of the ordinated space.
    A sub-sample (or subsamples) contains the same number of dimensions as the ordinated space but may contain a smaller subset of elements.
    For example, if our ordinated space is composed of birds and mammals (the elements) and 50 principal components (the dimensions), we can create two subsamples containing just mammals or birds, but with the same 50 dimensions, to compare disparity in the two clades.



# The package's guts

## Manipulating `dispRity` objects

Disparity analysis involves a lot of manipulation of many matrices (especially when bootstrapping) which can be impractical to visualise and will quickly overwhelm your `R` console.
Even the simple Beck and Lee 2014 example above produces an object with > 72 lines of lists of lists of matrices!

Therefore `dispRity` uses a specific class of object called a `dispRity` object.
These objects allow users to use S3 method functions such as `summary.dispRity`, `plot.dispRity` and `print.dispRity`. 
`dispRity` also contains various utility functions that manipulate the `dispRity` object (e.g. `sort.dispRity`, `extract.dispRity` see the full list in the next section).
These functions modify the `dispRity` object without having to delve into its complex structure!
The full structure of a `dispRity` object is detailed [here](https://github.com/TGuillerme/dispRity/blob/master/disparity_object.md).

```{r}
## Loading the example data
data(disparity)

## What is the class of the median_centroids object?
class(disparity)

## What does the object contain?
names(disparity)

## Summarising it using the S3 method print.dispRity
disparity
```

Note that it is always possible to recall the full object using the argument `all = TRUE` in `print.dispRity`:

```{r, eval=FALSE}
## Display the full object
print(disparity, all = TRUE)
## This is more nearly ~ 5000 lines on my 13 inch laptop screen!
```

## `dispRity` utilities

The package also provides some utilities functions to facilitate multidimensional analysis.

### `dispRity` object utilities  <a name="dispRity.utilities"></a>
The first subsamples of utilities are functions for manipulating `dispRity` objects:

#### `make.dispRity`
This function creates empty `dispRity` objects.

```{r}
## Creating an empty dispRity object
make.dispRity()

## Creating an "empty" dispRity object with a matrix
(disparity_obj <- make.dispRity(matrix(rnorm(20), 5, 4)))
```

####  `fill.dispRity`
This function initialises a `dispRity` object and generating it's call properties.

```{r}
## The dispRity object's call is indeed empty
disparity_obj$call

## Filling an empty disparity object (that needs to contain at least a matrix)
(disparity_obj <- fill.dispRity(disparity_obj))

## The dipRity object has now the proper minimal attributes
disparity_obj$call
```

####  `matrix.dispRity`
This function extracts a specific matrix from a disparity object. The matrix can be one of the bootstrapped or/and rarefied one.

```{r}
## Extracting the matrix containing the coordinates of the elements at time 50
str(matrix.dispRity(disparity, "50"))

## Extracting the 3rd boostrapped matrix with the 2nd rarefaction level
## (15 elements) from the second group (80 Mya)
str(matrix.dispRity(disparity, subsamples = 1, bootstrap = 3, rarefaction = 2))
```

#### `get.subsamples.dispRity`
This function creates a dispRity object that contains only elements from one specific subsamples.

```{r, eval=FALSE}
## Extracting all the data for the crown mammals
(crown_mammals <- get.subsamples.dispRity(disp_crown_stemBS, "Group.crown"))

## The object keeps the properties of the parent object but is composed of only one subsamples
length(crown_mammals$subsamples)
```

#### `extract.dispRity`
This function extracts the calculated disparity values of a specific matrix.

```{r, eval=FALSE}
## Extracting the observed disparity (default)
extract.dispRity(disparity)

## Extracting the disparity from the bootstrapped values from the
## 10th rarefaction level from the second subsamples (80 Mya)
extract.dispRity(disparity, observed = FALSE, subsamples = 2, rarefaction = 10)
```

#### `scale.dispRity`
This is the S3 method of `scale` (scaling and/or centring) that can be applied to the disparity data of a `dispRity` object.

```{r, eval=FALSE}
## Getting the disparity values of the time subsamples
head(summary(disparity))

## Scaling the same disparity values
head(summary(scale(disparity, scale = TRUE)))

## Scaling and centering:
head(summary(scale(disparity, scale = TRUE, center = TRUE)))
```

#### `sort.dispRity`
This is the S3 method of `sort` for sorting the subsamples alphabetically (default) or following a specific pattern.

```{r, eval=FALSE}
## Sorting the disparity subsamples in inverse alphabetic order
head(summary(sort(disparity, decreasing = TRUE)))

## Customised sorting
head(summary(sort(disparity, sort = c(7,1,3,4,5,2,6))))
```


<!-- ## Modularity

### Making your own metrics

### Making your own tests

## Where is this going?
 -->
<!-- ## Running `dispRity` in parallel

The computationally intensive function `dispRity` have a `parallel` option to speed up their calculations.

This option requires the package `snow` and takes arguments that are to be passed to  -->
